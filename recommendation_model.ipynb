{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "J5C05s5vX-55"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install scikit-learn\n",
        "# !pip install tensorflow\n",
        "# !pip install supabase\n",
        "# !pip install python-dotenv\n",
        "# !pip install matplotlib\n",
        "# !pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "06vXWSUr6P2y"
      },
      "outputs": [],
      "source": [
        "import time, numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import tensorflow as tf\n",
        "from supabase import create_client\n",
        "from dotenv import load_dotenv\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY_CcKumJHuC"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2PtfrnUVpI1",
        "outputId": "4d16d1a3-9845-45aa-afdb-a60ed53a5566"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "he3WgtROVsM4"
      },
      "outputs": [],
      "source": [
        "url = os.environ['SUPABASE_URL']\n",
        "key = os.environ['SUPABASE_KEY']\n",
        "\n",
        "supabase = create_client(url, key)\n",
        "\n",
        "data = supabase.table(\"liked_songs\").select(\"user_id, song_id, like_count\").execute()\n",
        "d = json.loads(data.json())\n",
        "\n",
        "# Storing the extracted data in a variable as a list of dictionaries\n",
        "liked_songs = d.get('data', [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "sQz0tJRD7Z46"
      },
      "outputs": [],
      "source": [
        "songs_data = pd.read_csv(\"/data/notesync-dataset-1k.csv\")\n",
        "liked_songs_data = pd.DataFrame(liked_songs)\n",
        "saved_user_id = liked_songs_data['user_id'].iloc[-1]\n",
        "recent_user_id = saved_user_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "X2Ii8XTgs-iD"
      },
      "outputs": [],
      "source": [
        "test_likes = pd.read_csv(\"/content/data/mixed_likes_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "UPc4EyTuA5K-"
      },
      "outputs": [],
      "source": [
        "# Mapping song_ids to integer IDs\n",
        "songs_data['mapped_song_id'] = songs_data['song_id'].astype('category').cat.codes\n",
        "liked_songs_data['mapped_song_id'] = liked_songs_data['song_id'].astype('category').cat.codes\n",
        "test_likes['mapped_song_id'] = test_likes['song_id'].astype('category').cat.codes\n",
        "\n",
        "# Mapping UUIDs to integer IDs\n",
        "unique_user_ids = liked_songs_data['user_id'].unique()\n",
        "# unique_user_ids = test_likes['user_id'].unique()\n",
        "user_id_map = {user_id: i for i, user_id in enumerate(unique_user_ids)}\n",
        "\n",
        "# Converting UUIDs in liked_songs_data to integer IDs\n",
        "liked_songs_data['user_id'] = liked_songs_data['user_id'].map(user_id_map)\n",
        "test_likes['user_id'] = test_likes['user_id'].map(user_id_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OETH3onGWJdR"
      },
      "source": [
        "# Content-Based Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "oE064PNosVpX"
      },
      "outputs": [],
      "source": [
        "def calculate_user_cosine_similarity(user_id, liked_songs, all_songs):\n",
        "    # Filter liked songs for the specified user\n",
        "    user_liked_songs = liked_songs[liked_songs['user_id'] == user_id]\n",
        "\n",
        "    if user_liked_songs.empty:\n",
        "        raise ValueError(\"No liked songs found for the user: \", user_id)\n",
        "\n",
        "    # Features extracted from dataset\n",
        "    features = ['speechiness', 'acousticness', 'instrumentalness', 'tempo']\n",
        "\n",
        "    user_liked_songs_features = user_liked_songs.merge(all_songs, on='song_id', how='inner')[features]\n",
        "\n",
        "    # Normalizing the scale to be between 0 and 1, this is used for specifically scaling the tempo since its range can get closer to 200\n",
        "    # This makes sure the scores are calculated accurately and aren't vastly different from each other\n",
        "    scaler = MinMaxScaler()\n",
        "    all_songs_scaled = scaler.fit_transform(all_songs[features])\n",
        "    user_liked_songs_scaled = scaler.transform(user_liked_songs_features)\n",
        "\n",
        "    similarity_scores = cosine_similarity(user_liked_songs_scaled, all_songs_scaled)\n",
        "\n",
        "    mean_similarity_scores = np.mean(similarity_scores, axis=0)\n",
        "\n",
        "    return mean_similarity_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq3jrPVNAOWC"
      },
      "source": [
        "# Collaborative Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "znnYi2i5qZns"
      },
      "outputs": [],
      "source": [
        "class MatrixFactorization():\n",
        "    def __init__(self, ratings, dimensions, alpha, beta, iterations):\n",
        "        self.ratings = ratings\n",
        "        self.num_users, self.num_songs = ratings.shape\n",
        "        self.dimensions = dimensions\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.iterations = iterations\n",
        "\n",
        "    def train_model(self):\n",
        "        # Initialize latent factors U (user) and S (song)\n",
        "        self.U = tf.Variable(tf.random.normal(shape=(self.num_users, self.dimensions), stddev=0.1/self.dimensions))\n",
        "        self.S = tf.Variable(tf.random.normal(shape=(self.num_songs, self.dimensions), stddev=0.1/self.dimensions))\n",
        "\n",
        "        # Training loop\n",
        "        for _ in tqdm.tqdm(range(self.iterations), desc=\"Training\"):\n",
        "            mse = self.stochastic_gd()\n",
        "            print(\" MSE: \", mse)\n",
        "\n",
        "    def stochastic_gd(self):\n",
        "        total_error = 0.0\n",
        "        num_ratings = 0\n",
        "\n",
        "        for i in range(self.num_users):\n",
        "            for j in range(self.num_songs):\n",
        "                if self.ratings[i, j] == 1:\n",
        "                    # Extract the prediction scores\n",
        "                    prediction = tf.reduce_sum(self.U[i, :] * self.S[j, :])\n",
        "\n",
        "                    # Compute the error values\n",
        "                    err = (self.ratings[i, j] - prediction)\n",
        "                    total_error += err**2\n",
        "                    num_ratings += 1\n",
        "\n",
        "                    # Updating the user latent factors\n",
        "                    update_U = tf.expand_dims(self.alpha * (err * self.S[j, :] - self.beta * self.U[i, :]), axis=0)\n",
        "                    indices_U = tf.expand_dims([i], axis=0)\n",
        "                    self.U = tf.tensor_scatter_nd_add(self.U, indices_U, update_U)\n",
        "\n",
        "                    # Updating the song latent factors\n",
        "                    update_S = tf.expand_dims(self.alpha * (err * self.U[i, :] - self.beta * self.S[j, :]), axis=0)\n",
        "                    indices_S = tf.expand_dims([j], axis=0)\n",
        "                    self.S = tf.tensor_scatter_nd_add(self.S, indices_S, update_S)\n",
        "\n",
        "        # Calculating the MSE for each iteration\n",
        "        mse = total_error / num_ratings\n",
        "        return mse\n",
        "\n",
        "    # Generate predicted ratings\n",
        "    def predict(self):\n",
        "        predictions = tf.matmul(self.U, tf.transpose(self.S))\n",
        "\n",
        "        # Min-max normalization\n",
        "        min_scores = tf.reduce_min(predictions)\n",
        "        max_scores = tf.reduce_max(predictions)\n",
        "        predictions_normalized = (predictions - min_scores) / (max_scores - min_scores)\n",
        "\n",
        "        return predictions_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_rVMjXCpUGa"
      },
      "outputs": [],
      "source": [
        "# Extracting the IDs\n",
        "user_ids = liked_songs_data['user_id'].unique()\n",
        "song_ids = songs_data['song_id']\n",
        "\n",
        "user_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
        "song_to_index = {song_id: index for index, song_id in enumerate(song_ids)}\n",
        "\n",
        "# Initializing the user-song matrix\n",
        "ratings_matrix = np.zeros((len(user_ids), len(song_ids)))\n",
        "\n",
        "# Filling the matrix with 1s where a user has liked a song\n",
        "for _, row in liked_songs_data.iterrows():\n",
        "    if row['song_id'] in song_to_index:\n",
        "        user_idx = user_to_index[row['user_id']]\n",
        "        song_idx = song_to_index[row['song_id']]\n",
        "        ratings_matrix[user_idx, song_idx] = 1\n",
        "\n",
        "# Generating the ratings matrix\n",
        "ratings_matrix.shape, np.sum(ratings_matrix)\n",
        "\n",
        "# Initialize and train matrix factorization model\n",
        "matrix_fz = MatrixFactorization(ratings_matrix, dimensions=15, alpha=0.05, beta=0.05, iterations=200)\n",
        "matrix_fz.train_model()\n",
        "\n",
        "# Generate predicted ratings\n",
        "predicted_ratings = matrix_fz.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBnR1fpMaRgK"
      },
      "source": [
        "# Hybrid Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "J1QHVR8QziYt"
      },
      "outputs": [],
      "source": [
        "def recommend_songs(user_id, n=20, alpha=0.5):\n",
        "    get_user_id = user_id_map[user_id]\n",
        "\n",
        "    # Content-based filtering scores\n",
        "    content_based_scores = calculate_user_cosine_similarity(get_user_id, liked_songs_data, songs_data)\n",
        "    print(\"\\n Content Based Scores: \", content_based_scores)\n",
        "\n",
        "    # Collaborative filtering scores\n",
        "    collaborative_scores = matrix_fz.predict()[get_user_id].numpy()\n",
        "    print(\"\\n Collaborative Scores: \", collaborative_scores)\n",
        "\n",
        "    # Ensuring the sizes of both scores are the same\n",
        "    assert len(content_based_scores) == len(collaborative_scores) == len(songs_data), \"Scores and songs dataset must match in length.\"\n",
        "\n",
        "    # Calculating hybrid scores\n",
        "    hybrid_scores = alpha * content_based_scores + (1 - alpha) * collaborative_scores\n",
        "    print(\"\\n Hybrid Scores: \", hybrid_scores)\n",
        "\n",
        "    # Adding the hybrid scores to the column\n",
        "    songs_data['hybrid_score'] = hybrid_scores\n",
        "\n",
        "    # Filtering out the songs that don't exist on the application\n",
        "    filtered_songs_data = songs_data.dropna(subset=['song_id'])\n",
        "\n",
        "    # Sorting the scores in descending order and taking the top n rows\n",
        "    recommended_songs_df = filtered_songs_data.sort_values(by='hybrid_score', ascending=False).head(n)\n",
        "    recommended_songs_df = recommended_songs_df[['hybrid_score', 'song_id', 'artist', 'track_name']]\n",
        "\n",
        "    return recommended_songs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWIYnSzn8tlw"
      },
      "outputs": [],
      "source": [
        "recommended_songs_hybrid = recommend_songs(recent_user_id)\n",
        "print(\"\\nHybrid recommended songs for user\", recent_user_id, \":\")\n",
        "print(recommended_songs_hybrid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI-FyhH5Z0g9"
      },
      "source": [
        "# Push Recommended Songs to Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "aXkxP-NiWoTg"
      },
      "outputs": [],
      "source": [
        "# song_ids = recommended_songs_hybrid['song_id'].values\n",
        "# artists = recommended_songs_hybrid['artist'].values\n",
        "\n",
        "# for song_id in song_ids:\n",
        "#     supabase.table(\"recommended_songs\").upsert({'user_id': recent_user_id, 'song_id': song_id}).execute()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
